{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78f7370",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Recognition: Training + Streamlit Inference\n",
    "\n",
    "This notebook follows your projectâ€™s structure to train a lightweight multimodal model on pre-extracted features (face AUs via OpenFace and audio features via openSMILE), evaluate it, save artifacts, and create a Streamlit app for testing.\n",
    "\n",
    "- Project spec source: `Emotion_recognition.md`\n",
    "- Inputs: `data/features/face_feats.npy`, `data/features/audio_feats.npy`, `data/features/labels.npy`\n",
    "- Output: `artifacts/ckpt.pt` with class names\n",
    "\n",
    "Dataset/resources links:\n",
    "- OpenFace: https://github.com/TadasBaltrusaitis/OpenFace\n",
    "- openSMILE: https://github.com/audeering/opensmile\n",
    "\n",
    "If you need a ready dataset to start, consider RAVDESS/TESS/CREMA-D for audio and AFEW/FER+ for video; youâ€™ll still extract features first as per the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cc191",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39561b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup: Dependencies\n",
    "# Installs are safe to re-run; Kaggle may have many preinstalled.\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pip_name or pkg])\n",
    "\n",
    "for py, pipn in [\n",
    "    ('numpy','numpy'), ('pandas','pandas'), ('scikit_learn','scikit-learn'),\n",
    "    ('torch','torch'), ('tqdm','tqdm'), ('joblib','joblib'), ('matplotlib','matplotlib'), ('seaborn','seaborn'),\n",
    "    ('streamlit','streamlit')\n",
    "]:\n",
    "    try:\n",
    "        __import__(py)\n",
    "    except Exception:\n",
    "        ensure(py, pipn)\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "print('Versions: torch', torch.__version__, '| numpy', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffd03e",
   "metadata": {},
   "source": [
    "## 2) Feature preparation (from spec)\n",
    "- Face features: `data/features/face_feats.npy` (e.g., OpenFace AU stats, shape [N, Ff])\n",
    "- Audio features: `data/features/audio_feats.npy` (e.g., openSMILE emobase, shape [N, Fa])\n",
    "- Labels: `data/features/labels.npy` (strings or ints, length N)\n",
    "\n",
    "If these files are missing, run your extraction scripts from `Emotion_recognition.md` or place the arrays in `data/features/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load features & quick EDA\n",
    "\n",
    "FACE_PATH = 'data/features/face_feats.npy'\n",
    "AUDIO_PATH = 'data/features/audio_feats.npy'\n",
    "LABELS_PATH = 'data/features/labels.npy'\n",
    "\n",
    "assert os.path.exists(FACE_PATH), f\"Missing {FACE_PATH}\"\n",
    "assert os.path.exists(AUDIO_PATH), f\"Missing {AUDIO_PATH}\"\n",
    "assert os.path.exists(LABELS_PATH), f\"Missing {LABELS_PATH}\"\n",
    "\n",
    "Xf = np.load(FACE_PATH)\n",
    "Xa = np.load(AUDIO_PATH)\n",
    "labels = np.load(LABELS_PATH, allow_pickle=True)\n",
    "\n",
    "assert len(Xf)==len(Xa)==len(labels), 'Mismatched lengths'\n",
    "N, Ff, Fa = len(labels), Xf.shape[1], Xa.shape[1]\n",
    "print('N=', N, 'face_dim=', Ff, 'audio_dim=', Fa)\n",
    "\n",
    "# Class distribution\n",
    "vals, counts = np.unique(labels, return_counts=True)\n",
    "print('Classes:', vals)\n",
    "print('Counts:', dict(zip(vals, counts)))\n",
    "\n",
    "# Basic stats\n",
    "print('Face feats mean/std per-dim sample:', float(Xf.mean()), float(Xf.std()))\n",
    "print('Audio feats mean/std per-dim sample:', float(Xa.mean()), float(Xa.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d084121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset and Model (from spec, simplified for single-GPU/CPU)\n",
    "\n",
    "class FeaturePairDataset(Dataset):\n",
    "    def __init__(self, Xf: np.ndarray, Xa: np.ndarray, labels: np.ndarray):\n",
    "        assert len(Xf)==len(Xa)==len(labels)\n",
    "        self.Xf = Xf.astype(np.float32)\n",
    "        self.Xa = Xa.astype(np.float32)\n",
    "        self.classes, y = np.unique(labels, return_inverse=True)\n",
    "        self.y = y.astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        xf = torch.from_numpy(self.Xf[idx])\n",
    "        xa = torch.from_numpy(self.Xa[idx])\n",
    "        y  = torch.tensor(self.y[idx])\n",
    "        return xf, xa, y\n",
    "\n",
    "class Branch(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, out_dim=128, p=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, feat_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim//2), nn.ReLU(),\n",
    "            nn.Linear(feat_dim//2, n_classes)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.cls(z)\n",
    "\n",
    "class AVFusion(nn.Module):\n",
    "    def __init__(self, in_face, in_audio, n_classes):\n",
    "        super().__init__()\n",
    "        self.face = Branch(in_face)\n",
    "        self.audio = Branch(in_audio)\n",
    "        self.fuse = FusionHead(128+128, n_classes)\n",
    "    def forward(self, xf, xa):\n",
    "        zf = self.face(xf)\n",
    "        za = self.audio(xa)\n",
    "        z = torch.cat([zf, za], dim=-1)\n",
    "        logits = self.fuse(z)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aec294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Train/Val split, training, evaluation\n",
    "\n",
    "ds = FeaturePairDataset(Xf, Xa, labels)\n",
    "num_classes = len(ds.classes)\n",
    "in_face = Xf.shape[1]\n",
    "in_audio = Xa.shape[1]\n",
    "\n",
    "val_ratio = 0.2\n",
    "val_size = max(1, int(len(ds)*val_ratio))\n",
    "train_size = len(ds) - val_size\n",
    "train_ds, val_ds = random_split(ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "BATCH = 128\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2)\n",
    "val_ld   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=2)\n",
    "\n",
    "model = AVFusion(in_face, in_audio, num_classes).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "best_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_ld, desc=f'ep{ep}')\n",
    "    for xf, xa, y in pbar:\n",
    "        xf, xa, y = xf.to(DEVICE), xa.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(xf, xa)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    tot = correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xf, xa, y in val_ld:\n",
    "            xf, xa, y = xf.to(DEVICE), xa.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(xf, xa)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            tot += y.numel()\n",
    "    acc = correct / max(1, tot)\n",
    "    print(f'val_acc={acc:.4f}')\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "print('Best val_acc:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b72789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Save artifacts\n",
    "\n",
    "ARTIFACTS = 'artifacts'\n",
    "os.makedirs(ARTIFACTS, exist_ok=True)\n",
    "CKPT = os.path.join(ARTIFACTS, 'ckpt.pt')\n",
    "META = os.path.join(ARTIFACTS, 'meta.json')\n",
    "\n",
    "if best_state is None:\n",
    "    best_state = model.state_dict()\n",
    "\n",
    "torch.save({'model_state': best_state, 'classes': ds.classes}, CKPT)\n",
    "with open(META, 'w') as f:\n",
    "    json.dump({'in_face': int(in_face), 'in_audio': int(in_audio)}, f)\n",
    "\n",
    "print('Saved:', CKPT, META)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Generate Streamlit test app (app.py)\n",
    "\n",
    "app_code = r\"\"\"\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(page_title='Multimodal Emotion Inference', page_icon='ðŸŽ­', layout='centered')\n",
    "\n",
    "ARTIFACTS = 'artifacts'\n",
    "CKPT = os.path.join(ARTIFACTS, 'ckpt.pt')\n",
    "META = os.path.join(ARTIFACTS, 'meta.json')\n",
    "\n",
    "blob = torch.load(CKPT, map_location='cpu')\n",
    "classes = [str(x) for x in blob['classes']]\n",
    "with open(META, 'r') as f:\n",
    "    meta = json.load(f)\n",
    "IN_FACE = int(meta['in_face'])\n",
    "IN_AUDIO = int(meta['in_audio'])\n",
    "\n",
    "# Define the same model structure\n",
    "import torch.nn as nn\n",
    "class Branch(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, out_dim=128, p=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, feat_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.LayerNorm(feat_dim),\n",
    "            nn.Linear(feat_dim, feat_dim//2), nn.ReLU(),\n",
    "            nn.Linear(feat_dim//2, n_classes)\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.cls(z)\n",
    "class AVFusion(nn.Module):\n",
    "    def __init__(self, in_face, in_audio, n_classes):\n",
    "        super().__init__()\n",
    "        self.face = Branch(in_face)\n",
    "        self.audio = Branch(in_audio)\n",
    "        self.fuse = FusionHead(256, n_classes)\n",
    "    def forward(self, xf, xa):\n",
    "        zf = self.face(xf)\n",
    "        za = self.audio(xa)\n",
    "        z = torch.cat([zf, za], dim=-1)\n",
    "        return self.fuse(z)\n",
    "\n",
    "model = AVFusion(IN_FACE, IN_AUDIO, len(classes))\n",
    "model.load_state_dict(blob['model_state'])\n",
    "model.eval()\n",
    "\n",
    "st.title('Multimodal Emotion Predictor')\n",
    "st.write('Enter or upload feature vectors to test the trained model.')\n",
    "\n",
    "mode = st.radio('Input mode', ['Manual sliders', 'Upload .npy vectors'])\n",
    "\n",
    "import torch\n",
    "if mode == 'Manual sliders':\n",
    "    st.subheader('Face features')\n",
    "    cols_f = st.columns(2)\n",
    "    face_vals = []\n",
    "    for i in range(IN_FACE):\n",
    "        with cols_f[i % 2]:\n",
    "            face_vals.append(st.number_input(f'face[{i}]', value=0.0))\n",
    "    st.subheader('Audio features')\n",
    "    cols_a = st.columns(2)\n",
    "    audio_vals = []\n",
    "    for i in range(IN_AUDIO):\n",
    "        with cols_a[i % 2]:\n",
    "            audio_vals.append(st.number_input(f'audio[{i}]', value=0.0))\n",
    "    if st.button('Predict'):\n",
    "        xf = torch.tensor(face_vals, dtype=torch.float32).unsqueeze(0)\n",
    "        xa = torch.tensor(audio_vals, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(xf, xa)\n",
    "            probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "        idx = int(np.argmax(probs))\n",
    "        st.metric('Predicted', classes[idx])\n",
    "        st.json({c: float(p) for c, p in zip(classes, probs)})\n",
    "else:\n",
    "    up_face = st.file_uploader('Upload face_feats.npy (1D vector or shape [F])', type=['npy'])\n",
    "    up_audio = st.file_uploader('Upload audio_feats.npy (1D vector or shape [F])', type=['npy'])\n",
    "    if st.button('Predict'):\n",
    "        if up_face is None or up_audio is None:\n",
    "            st.error('Please upload both vectors')\n",
    "        else:\n",
    "            xf = np.load(up_face)\n",
    "            xa = np.load(up_audio)\n",
    "            xf = torch.tensor(xf, dtype=torch.float32).view(1, -1)\n",
    "            xa = torch.tensor(xa, dtype=torch.float32).view(1, -1)\n",
    "            with torch.no_grad():\n",
    "                logits = model(xf, xa)\n",
    "                probs = torch.softmax(logits, dim=1).numpy()[0]\n",
    "            idx = int(np.argmax(probs))\n",
    "            st.metric('Predicted', classes[idx])\n",
    "            st.json({c: float(p) for c, p in zip(classes, probs)})\n",
    "\"\"\"\n",
    "\n",
    "with open('app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(app_code)\n",
    "print('Wrote app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c11f3",
   "metadata": {},
   "source": [
    "## 8) Run Streamlit app\n",
    "- After training cells have run and `artifacts/` exists with `ckpt.pt` and `meta.json`, launch:\n",
    "\n",
    "```\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- If features are large, prefer the file upload mode in the app.\n",
    "- Feature extraction tools:\n",
    "  - OpenFace: https://github.com/TadasBaltrusaitis/OpenFace\n",
    "  - openSMILE: https://github.com/audeering/opensmile"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
